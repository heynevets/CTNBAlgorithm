\section{Conclusions and Recommendations}


\subsection{summary and conclusions}
Our results from LDA were very unstable, changing significantly between each pass and as we changed the number of topics. Since we couldn’t find consistency in the topics, we can’t say with confidence that the topics that have the least support based on our scoring metrics are actually the topics with the least support. Thus, we can’t identify topics with limited support on Stack Overflow. However, we have found that our scoring metrics do work because older questions have more support as people have had more time to respond. This shows that we are working in the right direction, but need more time to develop the LDA model so that we can find a consistent set of topics.
\subsection{recommendations for future studies}
Future research can get better results by running our LDA models for a longer period of time (i.e. more passes to fit LDA). We were limited to 3 passes because of time constraints for fitting the model, but more passes would increase the accuracy. In addition, because fitting the LDA model took so long, further research can be done to explore different numbers of topics to fit LDA to and how that affects the results. We analyzed 20, 50, and 100 topics, but more or fewer topics may show insights that we have not seen before. Another area worth exploring would be how the number of stop words removes affects LDA accuracy. We removed a large amount of stop words, approximately 500,000, and this may have been too many. It is possible that removing fewer stop words would result in a more accurate model. One thing we did find is that numbers showed up quite frequently as top words in a topic, even though they often carry no specific meaning. In the future, we recommend adding numbers to the list of stopwords.

Another avenue of analysis would be to explore different scoring metrics to measure topic support. We noticed that for every topic, there were more answer upvotes than question upvotes, so our scoring metric was always negative. This is not bad in and of itself, but this may be an indicator that a different scoring metric may be better in evaluating support for a topic.

A major area of analysis that we were not able to touch was to correlate which topics have limited support with the job postings data for each year. The hypothesis is that topics with limited support will have more open jobs that year because there is a lack of talent in that topic. We could not find suitable job postings data that was categorized by year, so we weren’t able to perform this analysis. However, this is a very important area of research because if the hypothesis is true, then we can predict how many jobs in certain fields will remain open based on the related Stack Overflow questions and answers.
